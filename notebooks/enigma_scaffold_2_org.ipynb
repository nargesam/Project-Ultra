{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, GRU\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import enigma_model as em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = open('fra.txt', encoding='utf-8').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_samples = 1<<16\n",
    "# plain, cipher = em.generate_data(nb_samples)\n",
    "# enigma_data = pd.DataFrame({'PLAIN': plain, 'CIPHER': cipher})\n",
    "# enigma_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enigma_data.to_csv('./enigma_data_15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384\n",
      "16384\n"
     ]
    }
   ],
   "source": [
    "enigma_data = pd.read_csv('./enigma_data_15.csv')\n",
    "len(enigma_data)\n",
    "\n",
    "test = enigma_data.sample(n=16384, replace=False, random_state=42)\n",
    "enigma_data = enigma_data.drop(test.index)\n",
    "print(len(test))\n",
    "print(len(enigma_data))\n",
    "nb_samples = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PLAIN</th>\n",
       "      <th>CIPHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NOTICEKIDS</td>\n",
       "      <td>AMJAEZZGJN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>UPPORTDECI</td>\n",
       "      <td>RTZESCHLEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>DE</td>\n",
       "      <td>LO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>CHOICECHAR</td>\n",
       "      <td>VNBAEZMOQG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>ACTERTECHN</td>\n",
       "      <td>KUCOSCUMJY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>OLOGYSERIO</td>\n",
       "      <td>TKEIZUGLVU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>USLAND</td>\n",
       "      <td>MZJWJE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>OTHERLISTE</td>\n",
       "      <td>PBAKMUXWYQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>NMANAGEMEN</td>\n",
       "      <td>JHDQKEUCCY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>TAHEADCASE</td>\n",
       "      <td>OBFNULDZHW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID       PLAIN      CIPHER\n",
       "0   1  NOTICEKIDS  AMJAEZZGJN\n",
       "1   1  UPPORTDECI  RTZESCHLEE\n",
       "2   1          DE          LO\n",
       "3   2  CHOICECHAR  VNBAEZMOQG\n",
       "4   2  ACTERTECHN  KUCOSCUMJY\n",
       "5   2  OLOGYSERIO  TKEIZUGLVU\n",
       "6   2      USLAND      MZJWJE\n",
       "7   5  OTHERLISTE  PBAKMUXWYQ\n",
       "8   5  NMANAGEMEN  JHDQKEUCCY\n",
       "9   5  TAHEADCASE  OBFNULDZHW"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def divide_chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "df = enigma_data.copy()\n",
    "df\n",
    "\n",
    "enigma_data_partitions = {\n",
    "    'ID': [], \n",
    "    'PLAIN': [], \n",
    "    'CIPHER': []\n",
    "}\n",
    "\n",
    "n = 10\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    plain = list(str(row['PLAIN']))\n",
    "    cipher = list(str(row['CIPHER']))\n",
    "    \n",
    "    plain = list(divide_chunks(lst=plain, n=n))\n",
    "    cipher = list(divide_chunks(lst=cipher, n=n))\n",
    "    \n",
    "    for i in range(len(plain)):\n",
    "        plain_now = \"\".join(plain[i])\n",
    "        cipher_now = \"\".join(cipher[i])\n",
    "        \n",
    "        enigma_data_partitions['ID'].append(index)\n",
    "        enigma_data_partitions['PLAIN'].append(plain_now)\n",
    "        enigma_data_partitions['CIPHER'].append(cipher_now)\n",
    "    \n",
    "enigma_data_partitions = pd.DataFrame(enigma_data_partitions)\n",
    "enigma_data_partitions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enigma_test_run = enigma_data_partitions.query('ID <= 4000')\n",
    "\n",
    "# len(enigma_test_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sent = []\n",
    "fra_sent = []\n",
    "eng_chars = set()\n",
    "fra_chars = set()\n",
    "# nb_samples = enigma_data.shape[0]\n",
    "\n",
    "# Process english and french sentences\n",
    "for index, row in enigma_data_partitions.iterrows():\n",
    "#     eng_line = str(lines[line]).split('\\t')[0]\n",
    "    eng_line = str(row['CIPHER'])\n",
    "    \n",
    "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
    "#     fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
    "    fra_line = f\"\\t{str(row['PLAIN'])}\\n\"\n",
    "    \n",
    "    eng_sent.append(eng_line)\n",
    "    fra_sent.append(fra_line)\n",
    "    \n",
    "    for ch in eng_line:\n",
    "        if (ch not in eng_chars):\n",
    "            eng_chars.add(ch)\n",
    "            \n",
    "    for ch in fra_line:\n",
    "        if (ch not in fra_chars):\n",
    "            fra_chars.add(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49701 49701\n",
      "AMJAEZZGJN \tNOTICEKIDS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_sent), len(eng_sent))\n",
    "\n",
    "print(eng_sent[0], fra_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enigma_test_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-0b3cd011b99b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menigma_test_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enigma_test_run' is not defined"
     ]
    }
   ],
   "source": [
    "enigma_test_run.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "fra_chars = sorted(list(fra_chars))\n",
    "eng_chars = sorted(list(eng_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMJAEZZGJN\n",
      "NOTICEKIDS\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "['\\t', '\\n', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "print(eng_sent[0])\n",
    "print(fra_sent[0].strip())\n",
    "print(eng_chars)\n",
    "print(fra_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each english character - key is index and value is english character\n",
    "eng_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get english character given its index - key is english character and value is index\n",
    "eng_char_to_index_dict = {}\n",
    "\n",
    "for k, v in enumerate(eng_chars):\n",
    "    eng_index_to_char_dict[k] = v\n",
    "    eng_char_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to index each french character - key is index and value is french character\n",
    "fra_index_to_char_dict = {}\n",
    "\n",
    "# dictionary to get french character given its index - key is french character and value is index\n",
    "fra_char_to_index_dict = {}\n",
    "for k, v in enumerate(fra_chars):\n",
    "    fra_index_to_char_dict[k] = v\n",
    "    fra_char_to_index_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
    "max_len_fra_sent = max([len(line) for line in fra_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "12\n",
      "49701\n"
     ]
    }
   ],
   "source": [
    "print(max_len_eng_sent)\n",
    "print(max_len_fra_sent)\n",
    "\n",
    "print(len(eng_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 49701\n",
    "tokenized_eng_sentences = np.zeros(shape = (nb_samples,max_len_eng_sent,len(eng_chars)), dtype='float32')\n",
    "tokenized_fra_sentences = np.zeros(shape = (nb_samples,max_len_fra_sent,len(fra_chars)), dtype='float32')\n",
    "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the english and french sentences\n",
    "\n",
    "nb_samples = 49701\n",
    "for i in range(nb_samples):\n",
    "    for k,ch in enumerate(eng_sent[i]):\n",
    "        tokenized_eng_sentences[i,k,eng_char_to_index_dict[ch]] = 1\n",
    "        \n",
    "    for k,ch in enumerate(fra_sent[i]):\n",
    "        tokenized_fra_sentences[i,k,fra_char_to_index_dict[ch]] = 1\n",
    "\n",
    "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "        if k > 0:\n",
    "            target_data[i,k-1,fra_char_to_index_dict[ch]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "\n",
    "encoder_input = Input(shape=(None,len(eng_chars)))\n",
    "encoder_LSTM = LSTM(256,return_state = True)\n",
    "encoder_outputs, encoder_h, encoder_c = encoder_LSTM (encoder_input)\n",
    "encoder_states = [encoder_h, encoder_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "\n",
    "decoder_input = Input(shape=(None,len(fra_chars)))\n",
    "decoder_LSTM = LSTM(256,return_sequences=True, return_state = True)\n",
    "decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(fra_chars),activation='softmax')\n",
    "decoder_out = decoder_dense(decoder_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34790 samples, validate on 14911 samples\n",
      "Epoch 1/100\n",
      "34790/34790 [==============================] - 18s 530us/step - loss: 2.1885 - val_loss: 2.2747\n",
      "Epoch 2/100\n",
      "34790/34790 [==============================] - 15s 443us/step - loss: 2.0114 - val_loss: 1.9876\n",
      "Epoch 3/100\n",
      "34790/34790 [==============================] - 16s 454us/step - loss: 1.8750 - val_loss: 1.8030\n",
      "Epoch 4/100\n",
      "34790/34790 [==============================] - 16s 451us/step - loss: 1.7575 - val_loss: 1.7145\n",
      "Epoch 5/100\n",
      "34790/34790 [==============================] - 17s 500us/step - loss: 1.7866 - val_loss: 1.6619\n",
      "Epoch 6/100\n",
      "34790/34790 [==============================] - 17s 496us/step - loss: 1.6792 - val_loss: 1.6082\n",
      "Epoch 7/100\n",
      "34790/34790 [==============================] - 18s 518us/step - loss: 1.5868 - val_loss: 1.5746\n",
      "Epoch 8/100\n",
      "34790/34790 [==============================] - 18s 521us/step - loss: 1.4960 - val_loss: 1.4611\n",
      "Epoch 9/100\n",
      "34790/34790 [==============================] - 16s 467us/step - loss: 1.4359 - val_loss: 1.3913\n",
      "Epoch 10/100\n",
      "34790/34790 [==============================] - 16s 451us/step - loss: 1.3505 - val_loss: 1.3318\n",
      "Epoch 11/100\n",
      "34790/34790 [==============================] - 16s 458us/step - loss: 1.2677 - val_loss: 1.2437\n",
      "Epoch 12/100\n",
      "34790/34790 [==============================] - 16s 464us/step - loss: 1.1916 - val_loss: 1.1825\n",
      "Epoch 13/100\n",
      "34790/34790 [==============================] - 16s 451us/step - loss: 1.1245 - val_loss: 1.1215\n",
      "Epoch 14/100\n",
      "34790/34790 [==============================] - 17s 495us/step - loss: 1.0586 - val_loss: 1.0645\n",
      "Epoch 15/100\n",
      "34790/34790 [==============================] - 16s 457us/step - loss: 0.9950 - val_loss: 1.0102\n",
      "Epoch 16/100\n",
      "34790/34790 [==============================] - 18s 521us/step - loss: 0.9439 - val_loss: 0.9614\n",
      "Epoch 17/100\n",
      "34790/34790 [==============================] - 21s 608us/step - loss: 0.8807 - val_loss: 0.9138\n",
      "Epoch 18/100\n",
      "34790/34790 [==============================] - 21s 613us/step - loss: 0.8299 - val_loss: 0.8730\n",
      "Epoch 19/100\n",
      "34790/34790 [==============================] - 17s 477us/step - loss: 0.7871 - val_loss: 0.8536\n",
      "Epoch 20/100\n",
      "34790/34790 [==============================] - 17s 500us/step - loss: 0.7359 - val_loss: 0.7995\n",
      "Epoch 21/100\n",
      "34790/34790 [==============================] - 17s 502us/step - loss: 0.6923 - val_loss: 0.7684\n",
      "Epoch 22/100\n",
      "34790/34790 [==============================] - 18s 520us/step - loss: 0.6511 - val_loss: 0.7366\n",
      "Epoch 23/100\n",
      "34790/34790 [==============================] - 21s 612us/step - loss: 0.6097 - val_loss: 0.7001\n",
      "Epoch 24/100\n",
      "34790/34790 [==============================] - 18s 524us/step - loss: 0.5718 - val_loss: 0.6791\n",
      "Epoch 25/100\n",
      "34790/34790 [==============================] - 20s 574us/step - loss: 0.5357 - val_loss: 0.6498\n",
      "Epoch 26/100\n",
      "34790/34790 [==============================] - 18s 521us/step - loss: 0.5002 - val_loss: 0.6261\n",
      "Epoch 27/100\n",
      "34790/34790 [==============================] - 18s 524us/step - loss: 0.4678 - val_loss: 0.6050\n",
      "Epoch 28/100\n",
      "34790/34790 [==============================] - 17s 492us/step - loss: 0.4356 - val_loss: 0.5718\n",
      "Epoch 29/100\n",
      "34790/34790 [==============================] - 17s 485us/step - loss: 0.4023 - val_loss: 0.5521\n",
      "Epoch 30/100\n",
      "34790/34790 [==============================] - 18s 506us/step - loss: 0.3745 - val_loss: 0.5308\n",
      "Epoch 31/100\n",
      "34790/34790 [==============================] - 18s 518us/step - loss: 0.3442 - val_loss: 0.5103\n",
      "Epoch 32/100\n",
      "34790/34790 [==============================] - 18s 519us/step - loss: 0.3192 - val_loss: 0.4951\n",
      "Epoch 33/100\n",
      "34790/34790 [==============================] - 17s 491us/step - loss: 0.2953 - val_loss: 0.4717\n",
      "Epoch 34/100\n",
      "34790/34790 [==============================] - 17s 475us/step - loss: 0.2711 - val_loss: 0.4605\n",
      "Epoch 35/100\n",
      "34790/34790 [==============================] - 16s 467us/step - loss: 0.2498 - val_loss: 0.4462\n",
      "Epoch 36/100\n",
      "34790/34790 [==============================] - 17s 485us/step - loss: 0.2296 - val_loss: 0.4294\n",
      "Epoch 37/100\n",
      "34790/34790 [==============================] - 19s 533us/step - loss: 0.2085 - val_loss: 0.4133\n",
      "Epoch 38/100\n",
      "34790/34790 [==============================] - 17s 499us/step - loss: 0.1910 - val_loss: 0.3993\n",
      "Epoch 39/100\n",
      "34790/34790 [==============================] - 17s 497us/step - loss: 0.1729 - val_loss: 0.3939\n",
      "Epoch 40/100\n",
      "34790/34790 [==============================] - 17s 489us/step - loss: 0.1587 - val_loss: 0.3878\n",
      "Epoch 41/100\n",
      "34790/34790 [==============================] - 17s 502us/step - loss: 0.1461 - val_loss: 0.3785\n",
      "Epoch 42/100\n",
      "34790/34790 [==============================] - 18s 526us/step - loss: 0.1320 - val_loss: 0.3787\n",
      "Epoch 43/100\n",
      "34790/34790 [==============================] - 17s 497us/step - loss: 0.1194 - val_loss: 0.3743\n",
      "Epoch 44/100\n",
      "34790/34790 [==============================] - 18s 518us/step - loss: 0.1076 - val_loss: 0.3670\n",
      "Epoch 45/100\n",
      "34790/34790 [==============================] - 21s 598us/step - loss: 0.0964 - val_loss: 0.3593\n",
      "Epoch 46/100\n",
      "34790/34790 [==============================] - 19s 559us/step - loss: 0.0875 - val_loss: 0.3585\n",
      "Epoch 47/100\n",
      "34790/34790 [==============================] - 19s 536us/step - loss: 0.0795 - val_loss: 0.3567\n",
      "Epoch 48/100\n",
      "34790/34790 [==============================] - 18s 531us/step - loss: 0.0714 - val_loss: 0.3596\n",
      "Epoch 49/100\n",
      "34790/34790 [==============================] - 17s 503us/step - loss: 0.0666 - val_loss: 0.3498\n",
      "Epoch 50/100\n",
      "34790/34790 [==============================] - 18s 511us/step - loss: 0.0588 - val_loss: 0.3555\n",
      "Epoch 51/100\n",
      "34790/34790 [==============================] - 18s 521us/step - loss: 0.0527 - val_loss: 0.3534\n",
      "Epoch 52/100\n",
      "34790/34790 [==============================] - 18s 510us/step - loss: 0.0478 - val_loss: 0.3564\n",
      "Epoch 53/100\n",
      "34790/34790 [==============================] - 18s 508us/step - loss: 0.0460 - val_loss: 0.3568\n",
      "Epoch 54/100\n",
      "34790/34790 [==============================] - 17s 476us/step - loss: 0.0431 - val_loss: 0.3454\n",
      "Epoch 55/100\n",
      "34790/34790 [==============================] - 17s 480us/step - loss: 0.0367 - val_loss: 0.3479\n",
      "Epoch 56/100\n",
      "34790/34790 [==============================] - 17s 486us/step - loss: 0.0328 - val_loss: 0.3459\n",
      "Epoch 57/100\n",
      "34790/34790 [==============================] - 20s 576us/step - loss: 0.0282 - val_loss: 0.3439\n",
      "Epoch 58/100\n",
      "34790/34790 [==============================] - 19s 559us/step - loss: 0.0257 - val_loss: 0.3543\n",
      "Epoch 59/100\n",
      "34790/34790 [==============================] - 20s 566us/step - loss: 0.0277 - val_loss: 0.3619\n",
      "Epoch 60/100\n",
      "34790/34790 [==============================] - 20s 562us/step - loss: 0.0361 - val_loss: 0.3624\n",
      "Epoch 61/100\n",
      "34790/34790 [==============================] - 22s 646us/step - loss: 0.0313 - val_loss: 0.3451\n",
      "Epoch 62/100\n",
      "34790/34790 [==============================] - 26s 749us/step - loss: 0.0204 - val_loss: 0.3447\n",
      "Epoch 63/100\n",
      "34790/34790 [==============================] - 25s 731us/step - loss: 0.0137 - val_loss: 0.3366\n",
      "Epoch 64/100\n",
      "34790/34790 [==============================] - 21s 591us/step - loss: 0.0109 - val_loss: 0.3424\n",
      "Epoch 65/100\n",
      "34790/34790 [==============================] - 19s 557us/step - loss: 0.0092 - val_loss: 0.3415\n",
      "Epoch 66/100\n",
      "34790/34790 [==============================] - 19s 542us/step - loss: 0.0081 - val_loss: 0.3456\n",
      "Epoch 67/100\n",
      "34790/34790 [==============================] - 20s 567us/step - loss: 0.0074 - val_loss: 0.3500\n",
      "Epoch 68/100\n",
      "34790/34790 [==============================] - 19s 551us/step - loss: 0.0071 - val_loss: 0.3538\n",
      "Epoch 69/100\n",
      "34790/34790 [==============================] - 19s 550us/step - loss: 0.0069 - val_loss: 0.3593\n",
      "Epoch 70/100\n",
      "34790/34790 [==============================] - 18s 529us/step - loss: 0.0069 - val_loss: 0.3597\n",
      "Epoch 71/100\n",
      "34790/34790 [==============================] - 19s 539us/step - loss: 0.0066 - val_loss: 0.3685\n",
      "Epoch 72/100\n",
      "34790/34790 [==============================] - 19s 533us/step - loss: 0.1354 - val_loss: 0.3632\n",
      "Epoch 73/100\n",
      "34790/34790 [==============================] - 19s 535us/step - loss: 0.0441 - val_loss: 0.3202\n",
      "Epoch 74/100\n",
      "34790/34790 [==============================] - 18s 525us/step - loss: 0.0143 - val_loss: 0.3133\n",
      "Epoch 75/100\n",
      "34790/34790 [==============================] - 18s 527us/step - loss: 0.0077 - val_loss: 0.3121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "34790/34790 [==============================] - 17s 503us/step - loss: 0.0060 - val_loss: 0.3175\n",
      "Epoch 77/100\n",
      "34790/34790 [==============================] - 18s 522us/step - loss: 0.0054 - val_loss: 0.3208\n",
      "Epoch 78/100\n",
      "34790/34790 [==============================] - 18s 526us/step - loss: 0.0050 - val_loss: 0.3259\n",
      "Epoch 79/100\n",
      "34790/34790 [==============================] - 18s 526us/step - loss: 0.0047 - val_loss: 0.3315\n",
      "Epoch 80/100\n",
      "34790/34790 [==============================] - 19s 536us/step - loss: 0.0046 - val_loss: 0.3330\n",
      "Epoch 81/100\n",
      "34790/34790 [==============================] - 18s 522us/step - loss: 0.0045 - val_loss: 0.3376\n",
      "Epoch 82/100\n",
      "34790/34790 [==============================] - 19s 537us/step - loss: 0.0044 - val_loss: 0.3406\n",
      "Epoch 83/100\n",
      "34790/34790 [==============================] - 18s 530us/step - loss: 0.0042 - val_loss: 0.3424\n",
      "Epoch 84/100\n",
      "34790/34790 [==============================] - 18s 519us/step - loss: 0.0041 - val_loss: 0.3432\n",
      "Epoch 85/100\n",
      "34790/34790 [==============================] - 19s 542us/step - loss: 0.0041 - val_loss: 0.3495\n",
      "Epoch 86/100\n",
      "34790/34790 [==============================] - 19s 536us/step - loss: 0.0041 - val_loss: 0.3523\n",
      "Epoch 87/100\n",
      "34790/34790 [==============================] - 18s 527us/step - loss: 0.0043 - val_loss: 0.3604\n",
      "Epoch 88/100\n",
      "34790/34790 [==============================] - 18s 520us/step - loss: 0.0177 - val_loss: 0.5729\n",
      "Epoch 89/100\n",
      "34790/34790 [==============================] - 18s 518us/step - loss: 0.1354 - val_loss: 0.3234\n",
      "Epoch 90/100\n",
      "34790/34790 [==============================] - 18s 518us/step - loss: 0.0282 - val_loss: 0.2986\n",
      "Epoch 91/100\n",
      "34790/34790 [==============================] - 18s 530us/step - loss: 0.0113 - val_loss: 0.2904\n",
      "Epoch 92/100\n",
      "34790/34790 [==============================] - 18s 519us/step - loss: 0.0059 - val_loss: 0.2856\n",
      "Epoch 93/100\n",
      "34790/34790 [==============================] - 18s 509us/step - loss: 0.0044 - val_loss: 0.2891\n",
      "Epoch 94/100\n",
      "34790/34790 [==============================] - 18s 505us/step - loss: 0.0040 - val_loss: 0.2941\n",
      "Epoch 95/100\n",
      "34790/34790 [==============================] - 18s 507us/step - loss: 0.0038 - val_loss: 0.2975\n",
      "Epoch 96/100\n",
      "34790/34790 [==============================] - 18s 512us/step - loss: 0.0036 - val_loss: 0.3015\n",
      "Epoch 97/100\n",
      "34790/34790 [==============================] - 18s 514us/step - loss: 0.0034 - val_loss: 0.3045\n",
      "Epoch 98/100\n",
      "34790/34790 [==============================] - 18s 513us/step - loss: 0.0035 - val_loss: 0.3061\n",
      "Epoch 99/100\n",
      "34790/34790 [==============================] - 18s 519us/step - loss: 0.0034 - val_loss: 0.3103\n",
      "Epoch 100/100\n",
      "34790/34790 [==============================] - 19s 533us/step - loss: 0.0033 - val_loss: 0.3127\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb8d9778c10>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy')\n",
    "model.fit(x=[tokenized_eng_sentences,tokenized_fra_sentences], \n",
    "          y=target_data,\n",
    "          batch_size=256,\n",
    "          epochs=100,\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models for testing\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
    "        translated_sent += sampled_fra_char\n",
    "        \n",
    "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return translated_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49645"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def divide_chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "df = test.copy()\n",
    "df\n",
    "\n",
    "enigma_test_partitions = {\n",
    "    'ID': [], \n",
    "    'PLAIN': [], \n",
    "    'CIPHER': []\n",
    "}\n",
    "\n",
    "n = 10\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    plain = list(str(row['PLAIN']))\n",
    "    cipher = list(str(row['CIPHER']))\n",
    "    \n",
    "    plain = list(divide_chunks(lst=plain, n=n))\n",
    "    cipher = list(divide_chunks(lst=cipher, n=n))\n",
    "    \n",
    "    for i in range(len(plain)):\n",
    "        plain_now = \"\".join(plain[i])\n",
    "        cipher_now = \"\".join(cipher[i])\n",
    "        \n",
    "        enigma_test_partitions['ID'].append(index)\n",
    "        enigma_test_partitions['PLAIN'].append(plain_now)\n",
    "        enigma_test_partitions['CIPHER'].append(cipher_now)\n",
    "    \n",
    "enigma_test_partitions = pd.DataFrame(enigma_test_partitions)\n",
    "enigma_test_partitions.head(10)\n",
    "len(enigma_test_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples_test = 49645\n",
    "eng_sent_test = []\n",
    "fra_sent_test = []\n",
    "eng_chars_test = set()\n",
    "fra_chars_test = set()\n",
    "# nb_samples = enigma_data.shape[0]\n",
    "\n",
    "# Process english and french sentences\n",
    "for index, row in enigma_test_partitions.iterrows():\n",
    "#     eng_line = str(lines[line]).split('\\t')[0]\n",
    "    eng_line = str(row['CIPHER'])\n",
    "    \n",
    "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
    "#     fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
    "    fra_line = f\"\\t{str(row['PLAIN'])}\\n\"\n",
    "    \n",
    "    eng_sent_test.append(eng_line)\n",
    "    fra_sent_test.append(fra_line)\n",
    "    \n",
    "    for ch in eng_line:\n",
    "        if (ch not in eng_chars_test):\n",
    "            eng_chars_test.add(ch)\n",
    "            \n",
    "    for ch in fra_line:\n",
    "        if (ch not in fra_chars_test):\n",
    "            fra_chars_test.add(ch)\n",
    "fra_chars_test = sorted(list(fra_chars_test))\n",
    "eng_chars_test = sorted(list(eng_chars_test))\n",
    "\n",
    "# dictionary to index each english character - key is index and value is english character\n",
    "eng_index_to_char_dict_test = {}\n",
    "\n",
    "# dictionary to get english character given its index - key is english character and value is index\n",
    "eng_char_to_index_dict_test = {}\n",
    "\n",
    "for k, v in enumerate(eng_chars_test):\n",
    "    eng_index_to_char_dict_test[k] = v\n",
    "    eng_char_to_index_dict_test[v] = k\n",
    "\n",
    "max_len_eng_sent_test = max([len(line) for line in eng_sent_test])\n",
    "max_len_fra_sent_test = max([len(line) for line in fra_sent_test])\n",
    "\n",
    "\n",
    "\n",
    "tokenized_eng_sentences_test = np.zeros(shape = (nb_samples_test,max_len_eng_sent_test,len(eng_chars_test)), dtype='float32')\n",
    "tokenized_fra_sentences_test = np.zeros(shape = (nb_samples_test,max_len_fra_sent_test,len(fra_chars_test)), dtype='float32')\n",
    "target_data_test = np.zeros((nb_samples_test, max_len_fra_sent_test, len(fra_chars_test)),dtype='float32')\n",
    "\n",
    "\n",
    "# Vectorize the english and french sentences\n",
    "\n",
    "for i in range(nb_samples_test):\n",
    "    for k,ch in enumerate(eng_sent_test[i]):\n",
    "        tokenized_eng_sentences_test[i,k,eng_char_to_index_dict_test[ch]] = 1\n",
    "        \n",
    "#     for k,ch in enumerate(fra_sent_test[i]):\n",
    "#         tokenized_fra_sentences_test[i,k,fra_char_to_index_dict_test[ch]] = 1\n",
    "\n",
    "#         # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "#         if k > 0:\n",
    "#             target_data_test[i,k-1,fra_char_to_index_dict_test[ch]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: JEJLCWSG\n",
      "Decoded sentence: FATHERDI\n",
      "decode Org sentence: \tFATHERDI\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: OFBNBZBD\n",
      "Decoded sentence: SCOVERPR\n",
      "decode Org sentence: \tSCOVERPR\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: FMCONUCB\n",
      "Decoded sentence: ICEFIRHT\n",
      "decode Org sentence: \tICEFIRST\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: PJDKYWTT\n",
      "Decoded sentence: FACTREAD\n",
      "decode Org sentence: \tFACTREAD\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: IASHKLVJ\n",
      "Decoded sentence: WELLFUTU\n",
      "decode Org sentence: \tWELLFUTU\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: ZQCGVHYG\n",
      "Decoded sentence: REMANAGE\n",
      "decode Org sentence: \tREMANAGE\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: OLOH\n",
      "Decoded sentence: MENT\n",
      "decode Org sentence: \tMENT\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: PPVBCBXW\n",
      "Decoded sentence: OVEREFIS\n",
      "decode Org sentence: \tOVEREXIS\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: YFBFAWKZ\n",
      "Decoded sentence: TCOLDRAP\n",
      "decode Org sentence: \tTCOLDCAP\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: FHDQ\n",
      "Decoded sentence: ITAL\n",
      "decode Org sentence: \tITAL\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: JGQKYGCG\n",
      "Decoded sentence: FRIENDSI\n",
      "decode Org sentence: \tFRIENDMI\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: BHEGMKJO\n",
      "Decoded sentence: LITARYEA\n",
      "decode Org sentence: \tLITARYEA\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: VECGFX\n",
      "Decoded sentence: TLEAST\n",
      "decode Org sentence: \tTLEAST\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: JFSHRZOY\n",
      "Decoded sentence: FILLMEAN\n",
      "decode Org sentence: \tFILLMEAN\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: NQKFCASG\n",
      "Decoded sentence: HASETHRE\n",
      "decode Org sentence: \tHEALTHRE\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: GSYSC\n",
      "Decoded sentence: QUICE\n",
      "decode Org sentence: \tQUIRE\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: UAJAYAAP\n",
      "Decoded sentence: SETONTOA\n",
      "decode Org sentence: \tSETINTOA\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: HMEMUQYJ\n",
      "Decoded sentence: NYTHINGU\n",
      "decode Org sentence: \tNYTHINGU\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: LLVOTUXU\n",
      "Decoded sentence: SDFOOTIV\n",
      "decode Org sentence: \tSEFFORTF\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: EPLKESKZ\n",
      "Decoded sentence: IGHTHIAS\n",
      "decode Org sentence: \tIGHTFIRS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for seq_index in range(20):\n",
    "    inp_seq = tokenized_eng_sentences_test[seq_index:seq_index+1]\n",
    "    translated_sent = decode_seq(inp_seq)\n",
    "    print('-' * 100)\n",
    "    print('Input sentence:', eng_sent_test[seq_index])\n",
    "    print('Decoded sentence:', translated_sent.strip())\n",
    "    print('decode Org sentence:', fra_sent_test[seq_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()\n",
    "nb_samples_test = 49645\n",
    "\n",
    "\n",
    "predicted_cipher = []\n",
    "# actual_cipher = []\n",
    "\n",
    "# cipher = list(test['CIPHER'])\n",
    "\n",
    "for seq_index in range(nb_samples_test):\n",
    "    inp_seq = tokenized_eng_sentences_test[seq_index:seq_index+1]\n",
    "    translated_sent = decode_seq(inp_seq)\n",
    "    predicted_cipher.append(translated_sent)\n",
    "#     actual_cipher.append(fra_sent_test[seq_index])\n",
    "    \n",
    "#     print('-' * 100)\n",
    "#     print('Input sentence:', eng_sent_test[seq_index])\n",
    "#     print('Decoded sentence:', translated_sent.strip())\n",
    "#     print('Decoded original:', cipher[seq_index])\n",
    "\n",
    "   \n",
    "    \n",
    "# def str_score(str_a, str_b) :\n",
    "# #     if len(str_a) != len(str_b):\n",
    "# #         return 0\n",
    "\n",
    "#     n_correct = 0\n",
    "\n",
    "#     for a, b in zip(str_a, str_b):\n",
    "#         n_correct += int(a == b)\n",
    "#     # print(f\" n_correct {n_correct}\")\n",
    "#     # print(f\" len  {n_correct}\")\n",
    "\n",
    "#     return n_correct / len(str_a)\n",
    "\n",
    "    \n",
    "# def score(predicted_plain, correct_plain):\n",
    "#     correct = 0\n",
    "\n",
    "#     for p, c in zip(predicted_plain, correct_plain):\n",
    "#         print(p,c)\n",
    "#         exit()\n",
    "#         if str_score(p, c) > 0.8:\n",
    "#             correct += 1\n",
    "#     print(f\" correct {correct}\")\n",
    "#     print(f\" len correct_plain {len(correct_plain)}\")\n",
    "\n",
    "#     return correct / len(correct_plain)\n",
    "\n",
    "\n",
    "# print(predicted_cipher)\n",
    "# print(actual_cipher)\n",
    "\n",
    "\n",
    "# print(score(predicted_cipher, actual_cipher ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID       PLAIN      CIPHER\n",
      "0  1588  FATHERDISC  JEJLCWSGOF\n",
      "1  1588  OVERPRICEF  BNBZBDFMCO\n",
      "2  1588  IRSTFACTRE  NUCBPJDKYW\n",
      "3  1588          AD          TT\n",
      "4  2153  WELLFUTURE  IASHKLVJZQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['FATHERDISCOVERPRICEFIRSTACHEREAD',\n",
       " 'WELLFUTUREMANAGEMENT',\n",
       " 'OVEREXISTCOLDCAPITAL',\n",
       " 'FRIENDMILITARYEATLEAST',\n",
       " 'FILLMEANHEALTHRELWIRE',\n",
       " 'RETITTOANDTHINGUREFFORTFIGHTWIAST',\n",
       " 'THROUGHOUTBILLTARIF',\n",
       " 'STYLEPEOPLEFORCEPUTMODERN',\n",
       " 'ENOUGSUBLPARTYPARENTENOUGH',\n",
       " 'BAXPACTFACEALONEWITHOUT']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# predicted_cipher = pd.DataFrame(predicted_cipher)\n",
    "\n",
    "# predicted_cipher.save_csv('./predicted_english_seperated')\n",
    "\n",
    "print(enigma_test_partitions.head())\n",
    "\n",
    "predicted_cipher[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_cipher = predicted_cipher.iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-eff0bddf017e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menigma_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menigma_test_partitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menigma_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DECRYPTED'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_cipher\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menigma_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/enigma/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/enigma/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/enigma/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3636\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3638\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/enigma/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "enigma_predicted = enigma_test_partitions.copy()\n",
    "enigma_predicted['DECRYPTED'] = [x.strip() for x in predicted_cipher]\n",
    "enigma_predicted.head(20)\n",
    "\n",
    "\n",
    "enigma_predicted.to_csv('./enigma_predicted_n10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo = enigma_predicted.groupby(['ID'], as_index=False, sort=False) \\\n",
    "#     .agg(\n",
    "#     {\n",
    "#         'PLAIN': ''.join,\n",
    "#         'CIPHER': ''.join,\n",
    "#         'DECPRYPTED': ''.join\n",
    "#     })\n",
    "\n",
    "# foo = enigma_predicted[['ID', 'DECRYPTED']] \\\n",
    "#     .groupby(['ID'], as_index=False, sort=False) \\\n",
    "#     .agg(''.join)\n",
    "\n",
    "foo = enigma_predicted.groupby(['ID'], as_index=False, sort=False) \\\n",
    "    .agg(''.join)\n",
    "\n",
    "foo.head()\n",
    "\n",
    "foo.to_csv('./joined_predicted_cipher_n10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']})\n",
    "# df.groupby(['vid', 'sente'], as_index=False, sort=False).agg(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " correct 12145\n",
      " len correct_plain 16384\n",
      "0.74127197265625\n"
     ]
    }
   ],
   "source": [
    " \n",
    "def str_score(str_a, str_b) :\n",
    "    if len(str_a) != len(str_b):\n",
    "        return 0\n",
    "\n",
    "    n_correct = 0\n",
    "\n",
    "    for a, b in zip(str_a, str_b):\n",
    "        n_correct += int(a == b)\n",
    "    # print(f\" n_correct {n_correct}\")\n",
    "    # print(f\" len  {n_correct}\")\n",
    "\n",
    "    return n_correct / len(str_a)\n",
    "\n",
    "    \n",
    "def score(predicted_plain, correct_plain):\n",
    "    correct = 0\n",
    "\n",
    "    for p, c in zip(predicted_plain, correct_plain):\n",
    "#         print(p,c)\n",
    "#         exit()\n",
    "        if str_score(p, c) > 0.8:\n",
    "            correct += 1\n",
    "    print(f\" correct {correct}\")\n",
    "    print(f\" len correct_plain {len(correct_plain)}\")\n",
    "\n",
    "    return correct / len(correct_plain)\n",
    "\n",
    "\n",
    "# print(predicted_cipher)\n",
    "# print(actual_cipher)\n",
    "\n",
    "predicted_cipher = list(foo['DECRYPTED'])\n",
    "actual_cipher = list(foo['PLAIN'])\n",
    "print(score(predicted_cipher, actual_cipher ))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
