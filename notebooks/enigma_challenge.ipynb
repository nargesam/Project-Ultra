{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, GRU\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "import enigma_challenge as ec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAIN</th>\n",
       "      <th>CIPHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOLEARNABLEONE</td>\n",
       "      <td>HMSKLWYPLPTEVO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CARRYOTHERSASSUMEMEET</td>\n",
       "      <td>VEXBNYVOCGFGFXVBUCCIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELLNONEWITHACCEPT</td>\n",
       "      <td>UASHYYYVMHEMDWOGKH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GREENAIRBLACKLINEPUSHPAY</td>\n",
       "      <td>HGVKYTXDLPKUWVFYUVLRAHJW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PASSSEASONRESPONDMAYABLELESS</td>\n",
       "      <td>OELWWZOWSSUOFMCYHCDNHADNKXYY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          PLAIN                        CIPHER\n",
       "0                GOLEARNABLEONE                HMSKLWYPLPTEVO\n",
       "1         CARRYOTHERSASSUMEMEET         VEXBNYVOCGFGFXVBUCCIO\n",
       "2            SELLNONEWITHACCEPT            UASHYYYVMHEMDWOGKH\n",
       "3      GREENAIRBLACKLINEPUSHPAY      HGVKYTXDLPKUWVFYUVLRAHJW\n",
       "4  PASSSEASONRESPONDMAYABLELESS  OELWWZOWSSUOFMCYHCDNHADNKXYY"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power = 15\n",
    "n_samples = 1<<power\n",
    "\n",
    "save_file = f\"../data/raw/enigma_data_{power}.csv\"\n",
    "# ENIGMA_DATA = ec.generate_data(n_samples=n_samples, save_file=save_file)\n",
    "\n",
    "ENIGMA_OBJ = ec.EnigmaDataset(n_samples=n_samples, seq_len=42, save_file=save_file)\n",
    "ENIGMA_OBJ.dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test/train split (with caching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_samples = 16384\n",
    "sent_partition_size = 7\n",
    "\n",
    "ENIGMA_OBJ.test_train_split(n_test_samples=n_test_samples, \n",
    "                            sent_partition_size=sent_partition_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PLAIN</th>\n",
       "      <th>CIPHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CARRYOT</td>\n",
       "      <td>VEXBNYV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>HERSASS</td>\n",
       "      <td>OCGFGFX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>UMEMEET</td>\n",
       "      <td>VBUCCIO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>SELLNON</td>\n",
       "      <td>UASHYYY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>EWITHAC</td>\n",
       "      <td>VMHEMDW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    PLAIN   CIPHER\n",
       "0   1  CARRYOT  VEXBNYV\n",
       "1   1  HERSASS  OCGFGFX\n",
       "2   1  UMEMEET  VBUCCIO\n",
       "3   2  SELLNON  UASHYYY\n",
       "4   2  EWITHAC  VMHEMDW"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENIGMA_OBJ.train_data_partitioned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PLAIN</th>\n",
       "      <th>CIPHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1588</td>\n",
       "      <td>FINALLY</td>\n",
       "      <td>JFPIAUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1588</td>\n",
       "      <td>SUMMERA</td>\n",
       "      <td>WXYCOMH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1588</td>\n",
       "      <td>PPROACH</td>\n",
       "      <td>BZJKDVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1588</td>\n",
       "      <td>TENHOTE</td>\n",
       "      <td>XOENYOH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1588</td>\n",
       "      <td>LSHAKE</td>\n",
       "      <td>KIDCWL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID    PLAIN   CIPHER\n",
       "0  1588  FINALLY  JFPIAUN\n",
       "1  1588  SUMMERA  WXYCOMH\n",
       "2  1588  PPROACH  BZJKDVA\n",
       "3  1588  TENHOTE  XOENYOH\n",
       "4  1588   LSHAKE   KIDCWL"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENIGMA_OBJ.test_data_partitioned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training examples: 16384\n",
      "# of test examples: 16384\n"
     ]
    }
   ],
   "source": [
    "print(f\"# of training examples: {ENIGMA_OBJ.train_data.shape[0]}\")\n",
    "print(f\"# of test examples: {ENIGMA_OBJ.test_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENIGMA_ENCODED = ec.EncodedDataset(unencoded_dataset=ENIGMA_OBJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\tCARRYOT\\n', '\\tHERSASS\\n', '\\tUMEMEET\\n', '\\tSELLNON\\n', '\\tEWITHAC\\n']\n",
      "['VEXBNYV', 'OCGFGFX', 'VBUCCIO', 'UASHYYY', 'VMHEMDW']\n",
      "['\\tFINALLY\\n', '\\tSUMMERA\\n', '\\tPPROACH\\n', '\\tTENHOTE\\n', '\\tLSHAKE\\n']\n",
      "['JFPIAUN', 'WXYCOMH', 'BZJKDVA', 'XOENYOH', 'KIDCWL']\n"
     ]
    }
   ],
   "source": [
    "print(ENIGMA_ENCODED.plain_train.sentences_processed[0:5])\n",
    "print(ENIGMA_ENCODED.cipher_train.sentences_processed[0:5])\n",
    "print(ENIGMA_ENCODED.plain_test.sentences_processed[0:5])\n",
    "print(ENIGMA_ENCODED.cipher_test.sentences_processed[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\t', '\\n', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "['\\t', '\\n', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "print(ENIGMA_ENCODED.plain_train.alphabet)\n",
    "print(ENIGMA_ENCODED.cipher_train.alphabet)\n",
    "print(ENIGMA_ENCODED.plain_test.alphabet)\n",
    "print(ENIGMA_ENCODED.cipher_test.alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 1. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 1. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(ENIGMA_ENCODED.plain_train.target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Ultra Code Breaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra = ec.UltraCodeBreaker(plain_text=ENIGMA_ENCODED.plain_train, \n",
    "                            cipher_text=ENIGMA_ENCODED.cipher_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46674 samples, validate on 20004 samples\n",
      "Epoch 1/1\n",
      "46674/46674 [==============================] - 17s 371us/step - loss: 2.1550 - val_loss: 2.0141\n",
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_57 (InputLayer)           (None, None, 26)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_58 (InputLayer)           (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_57 (LSTM)                  [(None, 256), (None, 289792      input_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_58 (LSTM)                  [(None, None, 256),  291840      input_58[0][0]                   \n",
      "                                                                 lstm_57[0][1]                    \n",
      "                                                                 lstm_57[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, None, 28)     7196        lstm_58[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 588,828\n",
      "Trainable params: 588,828\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ultra.train(epochs=1, n_nodes=256)\n",
    "ultra.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra.create_test_model(n_nodes=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'UltraCodeBreaker' object has no attribute '_encoder_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-6a19914dc523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0multra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Data Challenges/Scale/src/enigma_challenge.py\u001b[0m in \u001b[0;36mcreate_test_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Encoder inference model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mencoder_model_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# Decoder inference model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UltraCodeBreaker' object has no attribute '_encoder_input'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the test model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "a\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to decode the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
    "        translated_sent += sampled_fra_char\n",
    "        \n",
    "        if ( (sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return translated_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide and conquer of the test data (DUPLICATED CODE!!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49645"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def divide_chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "df = test.copy()\n",
    "df\n",
    "\n",
    "enigma_test_partitions = {\n",
    "    'ID': [], \n",
    "    'PLAIN': [], \n",
    "    'CIPHER': []\n",
    "}\n",
    "\n",
    "n = 10\n",
    "\n",
    "for index, row in df.iterrows():    \n",
    "    plain = list(str(row['PLAIN']))\n",
    "    cipher = list(str(row['CIPHER']))\n",
    "    \n",
    "    plain = list(divide_chunks(lst=plain, n=n))\n",
    "    cipher = list(divide_chunks(lst=cipher, n=n))\n",
    "    \n",
    "    for i in range(len(plain)):\n",
    "        plain_now = \"\".join(plain[i])\n",
    "        cipher_now = \"\".join(cipher[i])\n",
    "        \n",
    "        enigma_test_partitions['ID'].append(index)\n",
    "        enigma_test_partitions['PLAIN'].append(plain_now)\n",
    "        enigma_test_partitions['CIPHER'].append(cipher_now)\n",
    "    \n",
    "enigma_test_partitions = pd.DataFrame(enigma_test_partitions)\n",
    "enigma_test_partitions.head(10)\n",
    "len(enigma_test_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create one-hot vectors for the test data (DUPLICATED CODE!!!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples_test = 49645\n",
    "eng_sent_test = []\n",
    "fra_sent_test = []\n",
    "eng_chars_test = set()\n",
    "fra_chars_test = set()\n",
    "# nb_samples = enigma_data.shape[0]\n",
    "\n",
    "# Process english and french sentences\n",
    "for index, row in enigma_test_partitions.iterrows():\n",
    "#     eng_line = str(lines[line]).split('\\t')[0]\n",
    "    eng_line = str(row['CIPHER'])\n",
    "    \n",
    "    # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
    "#     fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
    "    fra_line = f\"\\t{str(row['PLAIN'])}\\n\"\n",
    "    \n",
    "    eng_sent_test.append(eng_line)\n",
    "    fra_sent_test.append(fra_line)\n",
    "    \n",
    "    for ch in eng_line:\n",
    "        if (ch not in eng_chars_test):\n",
    "            eng_chars_test.add(ch)\n",
    "            \n",
    "    for ch in fra_line:\n",
    "        if (ch not in fra_chars_test):\n",
    "            fra_chars_test.add(ch)\n",
    "fra_chars_test = sorted(list(fra_chars_test))\n",
    "eng_chars_test = sorted(list(eng_chars_test))\n",
    "\n",
    "# dictionary to index each english character - key is index and value is english character\n",
    "eng_index_to_char_dict_test = {}\n",
    "\n",
    "# dictionary to get english character given its index - key is english character and value is index\n",
    "eng_char_to_index_dict_test = {}\n",
    "\n",
    "for k, v in enumerate(eng_chars_test):\n",
    "    eng_index_to_char_dict_test[k] = v\n",
    "    eng_char_to_index_dict_test[v] = k\n",
    "\n",
    "max_len_eng_sent_test = max([len(line) for line in eng_sent_test])\n",
    "max_len_fra_sent_test = max([len(line) for line in fra_sent_test])\n",
    "\n",
    "\n",
    "\n",
    "tokenized_eng_sentences_test = np.zeros(shape = (nb_samples_test,max_len_eng_sent_test,len(eng_chars_test)), dtype='float32')\n",
    "tokenized_fra_sentences_test = np.zeros(shape = (nb_samples_test,max_len_fra_sent_test,len(fra_chars_test)), dtype='float32')\n",
    "target_data_test = np.zeros((nb_samples_test, max_len_fra_sent_test, len(fra_chars_test)),dtype='float32')\n",
    "\n",
    "\n",
    "# Vectorize the english and french sentences\n",
    "\n",
    "for i in range(nb_samples_test):\n",
    "    for k,ch in enumerate(eng_sent_test[i]):\n",
    "        tokenized_eng_sentences_test[i,k,eng_char_to_index_dict_test[ch]] = 1\n",
    "        \n",
    "#     for k,ch in enumerate(fra_sent_test[i]):\n",
    "#         tokenized_fra_sentences_test[i,k,fra_char_to_index_dict_test[ch]] = 1\n",
    "\n",
    "#         # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "#         if k > 0:\n",
    "#             target_data_test[i,k-1,fra_char_to_index_dict_test[ch]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: JEJLCWSG\n",
      "Decoded sentence: FATHERDI\n",
      "decode Org sentence: \tFATHERDI\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: OFBNBZBD\n",
      "Decoded sentence: SCOVERPR\n",
      "decode Org sentence: \tSCOVERPR\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: FMCONUCB\n",
      "Decoded sentence: ICEFIRHT\n",
      "decode Org sentence: \tICEFIRST\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: PJDKYWTT\n",
      "Decoded sentence: FACTREAD\n",
      "decode Org sentence: \tFACTREAD\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: IASHKLVJ\n",
      "Decoded sentence: WELLFUTU\n",
      "decode Org sentence: \tWELLFUTU\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: ZQCGVHYG\n",
      "Decoded sentence: REMANAGE\n",
      "decode Org sentence: \tREMANAGE\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: OLOH\n",
      "Decoded sentence: MENT\n",
      "decode Org sentence: \tMENT\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: PPVBCBXW\n",
      "Decoded sentence: OVEREFIS\n",
      "decode Org sentence: \tOVEREXIS\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: YFBFAWKZ\n",
      "Decoded sentence: TCOLDRAP\n",
      "decode Org sentence: \tTCOLDCAP\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: FHDQ\n",
      "Decoded sentence: ITAL\n",
      "decode Org sentence: \tITAL\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: JGQKYGCG\n",
      "Decoded sentence: FRIENDSI\n",
      "decode Org sentence: \tFRIENDMI\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: BHEGMKJO\n",
      "Decoded sentence: LITARYEA\n",
      "decode Org sentence: \tLITARYEA\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: VECGFX\n",
      "Decoded sentence: TLEAST\n",
      "decode Org sentence: \tTLEAST\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: JFSHRZOY\n",
      "Decoded sentence: FILLMEAN\n",
      "decode Org sentence: \tFILLMEAN\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: NQKFCASG\n",
      "Decoded sentence: HASETHRE\n",
      "decode Org sentence: \tHEALTHRE\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: GSYSC\n",
      "Decoded sentence: QUICE\n",
      "decode Org sentence: \tQUIRE\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: UAJAYAAP\n",
      "Decoded sentence: SETONTOA\n",
      "decode Org sentence: \tSETINTOA\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: HMEMUQYJ\n",
      "Decoded sentence: NYTHINGU\n",
      "decode Org sentence: \tNYTHINGU\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: LLVOTUXU\n",
      "Decoded sentence: SDFOOTIV\n",
      "decode Org sentence: \tSEFFORTF\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input sentence: EPLKESKZ\n",
      "Decoded sentence: IGHTHIAS\n",
      "decode Org sentence: \tIGHTFIRS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for seq_index in range(20):\n",
    "    inp_seq = tokenized_eng_sentences_test[seq_index:seq_index+1]\n",
    "    translated_sent = decode_seq(inp_seq)\n",
    "    print('-' * 100)\n",
    "    print('Input sentence:', eng_sent_test[seq_index])\n",
    "    print('Decoded sentence:', translated_sent.strip())\n",
    "    print('decode Org sentence:', fra_sent_test[seq_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.head()\n",
    "nb_samples_test = 49645\n",
    "\n",
    "\n",
    "predicted_cipher = []\n",
    "# actual_cipher = []\n",
    "\n",
    "# cipher = list(test['CIPHER'])\n",
    "\n",
    "for seq_index in range(nb_samples_test):\n",
    "    inp_seq = tokenized_eng_sentences_test[seq_index:seq_index+1]\n",
    "    translated_sent = decode_seq(inp_seq)\n",
    "    predicted_cipher.append(translated_sent)\n",
    "    \n",
    "\n",
    "print(enigma_test_partitions.head())\n",
    "\n",
    "predicted_cipher[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the divide-and-conquery predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-eff0bddf017e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menigma_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menigma_test_partitions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menigma_predicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DECRYPTED'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredicted_cipher\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0menigma_predicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/enigma/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/enigma/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/enigma/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3636\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3638\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/enigma/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "enigma_predicted = enigma_test_partitions.copy()\n",
    "enigma_predicted['DECRYPTED'] = [x.strip() for x in predicted_cipher]\n",
    "enigma_predicted.head(20)\n",
    "\n",
    "\n",
    "enigma_predicted.to_csv('./enigma_predicted_n10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the predictions (and save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foo = enigma_predicted.groupby(['ID'], as_index=False, sort=False) \\\n",
    "#     .agg(\n",
    "#     {\n",
    "#         'PLAIN': ''.join,\n",
    "#         'CIPHER': ''.join,\n",
    "#         'DECPRYPTED': ''.join\n",
    "#     })\n",
    "\n",
    "# foo = enigma_predicted[['ID', 'DECRYPTED']] \\\n",
    "#     .groupby(['ID'], as_index=False, sort=False) \\\n",
    "#     .agg(''.join)\n",
    "\n",
    "foo = enigma_predicted.groupby(['ID'], as_index=False, sort=False) \\\n",
    "    .agg(''.join)\n",
    "\n",
    "foo.head()\n",
    "\n",
    "foo.to_csv('./joined_predicted_cipher_n10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " correct 12145\n",
      " len correct_plain 16384\n",
      "0.74127197265625\n"
     ]
    }
   ],
   "source": [
    " \n",
    "def str_score(str_a, str_b) :\n",
    "    if len(str_a) != len(str_b):\n",
    "        return 0\n",
    "\n",
    "    n_correct = 0\n",
    "\n",
    "    for a, b in zip(str_a, str_b):\n",
    "        n_correct += int(a == b)\n",
    "    # print(f\" n_correct {n_correct}\")\n",
    "    # print(f\" len  {n_correct}\")\n",
    "\n",
    "    return n_correct / len(str_a)\n",
    "\n",
    "    \n",
    "def score(predicted_plain, correct_plain):\n",
    "    correct = 0\n",
    "\n",
    "    for p, c in zip(predicted_plain, correct_plain):\n",
    "#         print(p,c)\n",
    "#         exit()\n",
    "        if str_score(p, c) > 0.8:\n",
    "            correct += 1\n",
    "    print(f\" correct {correct}\")\n",
    "    print(f\" len correct_plain {len(correct_plain)}\")\n",
    "\n",
    "    return correct / len(correct_plain)\n",
    "\n",
    "\n",
    "# print(predicted_cipher)\n",
    "# print(actual_cipher)\n",
    "\n",
    "predicted_cipher = list(foo['DECRYPTED'])\n",
    "actual_cipher = list(foo['PLAIN'])\n",
    "print(score(predicted_cipher, actual_cipher ))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
